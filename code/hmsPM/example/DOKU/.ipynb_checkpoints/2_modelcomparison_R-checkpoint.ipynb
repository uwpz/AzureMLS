{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook gives an example how we reach the decision which algorithm to choose for our final predictive model. It is the second notebook in a our series (of 5 notebooks in total) and relies on the results from the previous one, 1_explore_R.ipynb, which should be run in advance.  \n",
    "\n",
    "For a better understanding you should have **basic knowledge of predictive modelling**, i.e. know what a logistic regression is or cross-validation, for instance.  \n",
    "\n",
    "As example data source the well known **titanic data set** is chosen (https://www.kaggle.com/c/titanic/data). It is therefore a classification problem with the binary target *survived/not survived*. Even though you cannot compare the titanic case with a real life setting, the following steps pretty much reflect (on a small scale!) how we conduct data analysis in all our predictive modeling projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 0 Initialization\n",
    "\n",
    "We start by loading  the results from this preceeding notebook. Then we do some basic settings and load packages (including the important hmsPM package comprising highly helpful functions used in the following) by sourcing 0_init.R in the exact same way we did in 1_explore_R.ipynb.\n",
    "\n",
    "After that we initialize a cluster in order to process the forthcoming steps in parallel where possible. E.g all the tuning steps are done by utilizing the caret package which automatically conducts the tuning in parallel as long as a cluster is registered. Below we use just 4 processors on a 4-core laptop. Usually you run the following steps on a server or virtual machine of at least 16 cores as a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result from exploration\n",
    "setwd(\"C:/MY/hmsPM_R/hmsPM/inst/example\") \n",
    "load(paste0(\"data/\",\"1_explore.rdata\"))\n",
    "\n",
    "# Load libraries and functions\n",
    "suppressMessages(source(\"./code/0_init.R\"))\n",
    "\n",
    "# Initialize parallel processing\n",
    "cl = makeCluster(as.numeric(Sys.getenv(\"NUMBER_OF_PROCESSORS\")))\n",
    "registerDoParallel(cl)\n",
    "#clusterEvalQ(cl, library(hmsPM))\n",
    "# stopCluster(cl); closeAllConnections() #stop cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      age            fare_LOG_        fare_pp            pclass   \n",
       " Min.   : 0.1667   Min.   :0.000   Min.   : 0.00   1st      :323  \n",
       " 1st Qu.:21.0000   1st Qu.:2.186   1st Qu.: 7.55   2nd      :277  \n",
       " Median :28.0000   Median :2.738   Median : 8.05   3rd      :709  \n",
       " Mean   :29.9080   Mean   :2.977   Mean   :14.45   (Missing):  0  \n",
       " 3rd Qu.:39.0000   3rd Qu.:3.474   3rd Qu.:15.03                  \n",
       " Max.   :65.0000   Max.   :5.574   Max.   :55.44                  \n",
       "                                                                  \n",
       "        sex          sibsp         parch             embarked  \n",
       " female   :466   0      :891   0      :1002   Southampton:914  \n",
       " male     :843   1      :319   1      : 170   Cherbourg  :270  \n",
       " (Missing):  0   2      : 42   2      : 113   Queenstown :123  \n",
       "                 4      : 22   3      :   8   (Missing)  :  2  \n",
       "                 3      : 20   4      :   6                    \n",
       "                 8      :  9   5      :   6                    \n",
       "                 (Other):  6   (Other):   4                    \n",
       "                home.dest          deck        familysize       MISS_age   \n",
       " _OTHER_             :601   (Missing):1014   1      :790   no_miss  :1046  \n",
       " (Missing)           :564   C        :  94   2      :235   miss     : 263  \n",
       " New York, NY        : 64   B        :  65   3      :159   (Missing):   0  \n",
       " London              : 14   D        :  46   4      : 43                   \n",
       " Montreal, PQ        : 10   E        :  41   6      : 25                   \n",
       " Cornwall / Akron, OH:  9   A        :  22   5      : 22                   \n",
       " (Other)             : 47   (Other)  :  27   (Other): 35                   \n",
       "   MISS_fare_LOG_\n",
       " no_miss  :1308  \n",
       " miss     :   1  \n",
       " (Missing):   0  \n",
       "                 \n",
       "                 \n",
       "                 \n",
       "                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(df[features_notree])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 1 Test an algorithm (and determine parameter grid)  \n",
    "In this section we test different algorithms and determine a rough paramter grid we will be using in the model comparison section.\n",
    "<br>\n",
    "## 1.1 Sample data\n",
    "We start by sampling from our data in order to do a quick check of the following processing. Once we have finished that and are sure that everything works correctly we can increase the sampling rate or even use the full data (actually for titanic data there is no difference at all and the sampling is only for illustrating purpose).  \n",
    "\n",
    "Be aware that we are not doing a simple random sampling but already honor the usual situation of **unbalanced data** in a classification task, i.e. where the interesting class is underrepresented (i.e. the minority class). For titanic data the imbalance is small (38% survived) but in real life you often face highly imbalanced data (e.g. approx. 6% claim rate in car insurance).  \n",
    "The **undersampling** in the following step takes as many data from the minority class (which comprises the distinguishing information) as well as from the majority class **but** *n_max_per_level* at most, which lead to an undersampling of the majority class in case this parameter has a low value. Usually you take all the data from the minority class and add at least the same amount from the majority class (to get balanced data) or even more which would result in unbalanced data again but not as unbalanced as the original data.  \n",
    "\n",
    "Here we undersample the whole data. In the following code snippet you see some commented lines which reflect the approach to only undersample the training data but keep the test data as it is. Actually you only need the undersampling for faster training by not using all the data in hand which is often unnecessary with regard to the majority class.  \n",
    "Some people share the opinion that you need to balance your data in case you use tree-based algorithms. Actually this is not our experience as long as you care about proper tuning of your algorithm. Therefore our single reason for undersampling is to make the training of the model faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  N   Y \n",
      "500 500 \n",
      "  N   Y \n",
      "0.5 0.5 \n",
      "       N        Y \n",
      "0.618029 0.381971 \n"
     ]
    }
   ],
   "source": [
    "  # Sample from all data (take all but n_maxpersample at most)\n",
    "  c(df.tune, b_sample, b_all) %<-%  (df %>% hmsPM::undersample_n(n_max_per_level = 5e2))\n",
    "  # # Undersample only training data\n",
    "  # c(df.tmp, b_sample, b_all) %<-%  (df %>% filter(fold == \"train\") %>% hmsPM::undersample_n(n_max_per_level = 5e3)) \n",
    "  # df.tune = bind_rows(df.tmp, df %>% filter(fold == \"test\"))\n",
    "\n",
    "  print(summary(df.tune$target)); print(b_sample); print(b_all) #print new target distribution together with the new and old imbalance rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "## 1.2 Define possible cross-validation strategies\n",
    "Before we start to test (and tune) our algorithms, where we higly rely on the **caret R-package** (http://topepo.github.io/caret/index.html), we need to define the cross-validation strategy. Usually for tuning we do not use the gold standard of 5-fold as this takes too long to process and decide for a \"1-fold\", i.e. just take already defined (in 1_explore_R.ipynb) test data as the hold-out fold. This is reflected in the following *ctrl_idx* which is later used as control setting for the tuning.  \n",
    "We additionally already define some alternative control sets which refuses any **parallel tuning processing**. We need this control for a deep learning algorithm for instance or the standard \"xgbTree\" caret wrapper in conjunction with a sparse matrix which cannot be done in parallel, see also explanation below. We also add the 5-fold cross validation for the rare cases we would like to check the variability in the tuning results.\n",
    "\n",
    "All these predefined controls use the general *hmsPM::performance_summary* function to calculate the performance metrics and can be used for regression and classification as well.  \n",
    "\n",
    "One more hint regarding the \"fff\" suffixed controls: By using the indexFinal option in the trainControl function we tell the train function which is doing the tuning in a later step to process the final fit with the best tuning parameter set just on a small subset of data in order to make this unnecessary step (as we only want to find the best tuning parameter) as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.index = list(i = which(df.tune$fold == \"train\")) #take predefined train set\n",
    "#set.seed(998); #l.index = list(i = sample(1:nrow(df.tune), floor(0.8*nrow(df.tune)))) #random train set as alternative\n",
    "\n",
    "# Index based test-set\n",
    "ctrl_idx = trainControl(method = \"cv\", number = 1, index = l.index,\n",
    "                        returnResamp = \"final\", returnData = FALSE,\n",
    "                        summaryFunction = hmsPM::performance_summary, classProbs = TRUE)\n",
    "\n",
    "# Dito but \"fast\" final fit: DO NOT USE in case of further application!!!\n",
    "ctrl_idx_fff = ctrl_idx\n",
    "ctrl_idx_fff$indexFinal = sample(1:nrow(df.tune), 100) #\"Fast\" final fit!!!\n",
    "\n",
    "# Dito but without parallel processing: Needed for Lightgbm or DeepLearn\n",
    "ctrl_idx_nopar_fff = ctrl_idx_fff\n",
    "ctrl_idx_nopar_fff$allowParallel = FALSE\n",
    "\n",
    "# Dito as 5-fold cv\n",
    "ctrl_cv_fff = trainControl(method = \"cv\", number = 5,\n",
    "                           returnResamp = \"final\", returnData = FALSE,\n",
    "                           summaryFunction = hmsPM::performance_summary, classProbs = TRUE,\n",
    "                           indexFinal = sample(1:nrow(df.tune), 100)) #\"Fast\" final fit!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "## 1.3 Test algorithms\n",
    "Now we can test some algorithms and determine the best tuning parameter set.  \n",
    "\n",
    "We always restrict ourselves to the following algorithms: Elastic net, Boosted Trees, Deep Nets. Our experience it that you can soften the popular \"No free lunch theorem\" (which tells you that there is no single best algorithm for all predictive modeling tasks) to a \"at least ther is a free breakfast theorem\" as Boosted Trees has shown extraordinary performance for tabular data (and Deep nets for image and text data) also in public competitions (www.kaggle.com). Further reasons behind our selection are the following:\n",
    "- **Elastic Net** is fully interpretable as it provides you standard regression coefficients (the \"betas\") because it is basically a standard regression model with a small adaption of the loss function which makes it robust against irrelevant or collinear feature; therefore it is a perfect model candidate in real life (big data) situations. If using binned versions of metric variables it is also robust against outliers, skewness and missing values (see also 1_explore_R.ipynb) and also nonlinear effects can be approximated. This gives this algorithm a fair chance for competing with higly flexible black-box algorithms like Boosted Trees or Deep Nets.\n",
    "- **Boosted Trees** is still the best algorithm regarding predictive performance in case of \"tabular data\" (no image, speech or text processing) and therefore also the most popular approach on kaggle (www.kaggle.com) for this kind of data, which is not only a result of its predictive performance but also due to its robustness regarding missing values, skewed features, irrelevant or corrlated features, nonlinear effects as well as interaction effects (\"effect of age is differnt for male and female\"). Basically it is a \"sum of trees\" where the next tree is fitted on the residuals of the current ensemble before added to the ensemble in a shrinked fashion (i.e. the prediction of each tree is multiplied with a small value, the \"shrinkage\" parameter). Interestingly, even though it is more a black box algorithm due to the huge amount of trees that are fitted, one can make it nearly fully interpretable when parsing all trees and sum up the feature effects (see the forthcoming 3_interpretation_R.ipynb notebook). Regarding predictive performance it is usually also better than Random Forest, the second popular tree ensemble method, where the ensemble of trees is created by averaging the predictions of all trees in the ensemble. Below we additionally show the tuning for this algorithm as well.\n",
    "- **Deep Nets** as the pop star of Machine Learning is definitely the best algorithm for image and speech recoginition but (currently) not for non-receptive tasks like standard predictive modeling using tabular data. But it will probably catch up in future also for these tasks. For all these reasons it is recommeded to get familiar with this algorithm especially with the tuning of the Deep Net as this is still no pleasure at all. Deep Nets are Neural Net algorithms with more than 1 layer which have improved in the last decade due to new estimation algorithms with better regularization approaches. Very important for image and speech recognition tasks are special network architectures like convolutional nets and recurrent nets; for tabular the standard architecture is still composed of fully connected layers. \n",
    "\n",
    "There are different strategies to find the best tuning parameter. We almost always use a **grid search** starting with a rough grid and narrow it in consecutive steps which is guided by plotting the predictive performance for the tuning grid. If you have some experience with the algorithms you tune, this is still the best option from our perspective. If you are unfamiliar with the current algorithm at hand, a **random sample search** over a detailed grid might be more effective. There are more ambitious parameter search algorithms out there like Bayesian Optimization and others which we seldom give a try due to effort reasons. \n",
    "\n",
    "One more hint: It is absolutely important to plot the tuning results and do not use something like \"early stopping\" as it might happen in the latter case that you make your algorithm unnecessary complicated, e.g. when you receive a better performance by increasing the number of trees in a boosted tree algorithm up to several thousand but receive just a very small improvement regarding predictive performance compared to using just a few hundred trees. This can best be avoided by inspecting the **plot of the performance vs. the tuning parameter grid**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Elastic net\n",
    "We start with the \"glmnet\" which is the elastic-net algorithm, i.e. a penalized regression. Here we use the **binned metric variables** which are therefore all categorical in order to help the algorithm to fit also non-linear effects. In case you have a big dataset the creation of the design matrix from these binned variables will comprise a high memory amount due to the 1-hot coding which is done internally by the *train* caret-function. Remind from *1_explor_r.ipynb* that we cannot use any numerical encoding for a \"linear\" algorithm like elastic net. To keep memory consumption manageable for real life data we can hand over a **sparse matrix** which is much smaller than a dense one due to the many zeros resulting form the 1-hot coding. Unfortunately the standard \"glmnet\" wrapper provided by the caret package retransforms this sparse to a dense matrix again; so we adapted this wrapper and created a new *glmnet_custom* wrapper which does not do this retransformation. \n",
    "Be aware that no additional feature scaling is needed (as usual for an elastic net approach) as all features are scaled between 0 and 1 due to the 1-hot coding.\n",
    "\n",
    "The plot shows that alpha = 0 and lambda = 0.25 are the best parameter set. This is a little uncommon compred to real data where a pure *lasso penalization* (alpha = 1) usually outperforms a pure *ridge penalization* (alpha = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\""
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHgCAMAAABKCk6nAAAAQlBMVEUAAAAAZAAAgP9NTU1o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\nAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Lasso / Elastic Net\n",
    "fit = train(x         = sparse.model.matrix(as.formula(formula_binned),\n",
    "                                            df.tune[c(\"target\",features_binned)]),\n",
    "            y         = df.tune$target,\n",
    "            method    = glmnet_custom,\n",
    "            trControl = ctrl_idx_fff,\n",
    "            metric    = \"AUC\",\n",
    "            tuneGrid  = expand.grid(alpha = c(0,0.2,0.5,0.7,1),\n",
    "                                    lambda = 2^(seq(0, -15, -2))))\n",
    "            #preProc = c(\"center\",\"scale\")) #no scaling needed due to dummy coding of all variables\n",
    "options(repr.plot.width = 4, repr.plot.height = 4)\n",
    "plot(fit)\n",
    "\n",
    "# -> keep alpha=1 to have a full Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Boosted Trees\n",
    "We usually apply the **xgboost** implementation of boosted trees.  \n",
    "\n",
    "Unfortunately the standard caret wrapper \"xgbTree\" does not support parallel processing for tuning in conjunction with a **sparse matrix**. We therefore adapted this in the \"xgb_custom\" wrapper in order to be able to utilize the advantages of a sparse feature matrix.  \n",
    "\n",
    "Be aware that the tuning grid in the following snippet is far too ambitious for titanic but provides a good start for grid search with real life data. For the titanic data the tuning result shows some instable behaviour due to the small data amount.\n",
    "\n",
    "We also created an own plot function in the hmsPM package for the fit results as the standard output from the caret package cannot be always conveniently read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAAHgCAMAAABdO/S2AAAAQlBMVEUAAAAAv8QaGhozMzNN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\nCKpY/wd4JqVoM++meAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overwritten xgbTree: additional alpha and lambda parameter. Possible to use sparse matrix and parallel processing\n",
    "fit = train(x         = sparse.model.matrix(as.formula(formula), \n",
    "                                            df.tune[c(\"target\",features)]), \n",
    "            y         = df.tune$target,\n",
    "            method    = xgb_custom,\n",
    "            trControl = ctrl_idx_fff, #parallel for overwritten xgb\n",
    "            metric    = \"AUC\",\n",
    "            tuneGrid  = expand.grid(nrounds = seq(100,3100,200), eta = c(0.01,0.05),\n",
    "                                    max_depth = c(3,6), min_child_weight = c(10),\n",
    "                                    colsample_bytree = c(0.3, 0.7), subsample = c(0.7),\n",
    "                                    gamma = 0, alpha = 0, lambda = 1))\n",
    "options(repr.plot.width = 8, repr.plot.height = 4)\n",
    "hmsPM::plot_caret_result(fit, metric = \"AUC\", x = \"nrounds\",\n",
    "                  color = \"max_depth\", linetype = \"eta\", shape = \"min_child_weight\",\n",
    "                  facet = \"min_child_weight ~ subsample + colsample_bytree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next commented snippet we show how to use another popular boosted tree implementation, i.e. ***lightgbm*** for which we have also written an own wrapper to use with the caret package. Furthermore an example of using the ***ranger*** implementation of a Random Forest (which is much faster than the standard *randomForest*) is listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:27:28: unexpected ')'\n26: #                                    min.node.size = c(1,5,10)),\n27:             num.trees = 500)\n                               ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:27:28: unexpected ')'\n26: #                                    min.node.size = c(1,5,10)),\n27:             num.trees = 500)\n                               ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# # Lightgbm\n",
    "#  fit = train(x         = df.tune[features],\n",
    "#              y         = df.tune$target,\n",
    "#              #fit = train(sparse.model.matrix(as.formula(formula), df.tune[c(\"target\",features)]), df.tune$target,\n",
    "#              method    = lgbm,\n",
    "#              trControl = ctrl_idx_nopar_fff,\n",
    "#              metric    = \"AUC\",\n",
    "#              tuneGrid  = expand.grid(nrounds = seq(100,2100,200), learning_rate = c(0.01),\n",
    "#                                      num_leaves = 32, min_data_in_leaf = c(10),\n",
    "#                                      feature_fraction = c(0.7), bagging_fraction = c(0.7)),\n",
    "#              #max_depth = 3, #use for small data\n",
    "#              verbose   = -1)\n",
    " \n",
    "\n",
    "## Random Forest as alternative\n",
    "#fit = train(x         = df.tune[features],\n",
    "#            y         = df.tune$target,\n",
    "#            #fit = train(x = model.matrix(as.formula(formula), df.tune[c(\"target\",features)]), y = df.tune$target,\n",
    "#            method    = \"ranger\",\n",
    "#            trControl = ctrl_idx_fff,\n",
    "#            metric    = \"AUC\",\n",
    "#            tuneGrid  = expand.grid(mtry = seq(1,length(features),10),\n",
    "#                                    splitrule = switch(TARGET_TYPE,\n",
    "#                                                       \"CLASS\" = \"gini\", \"REGR\" = \"variance\",\n",
    "#                                                       \"MULTICLASS\" = \"gini\") ,\n",
    "#                                    min.node.size = c(1,5,10)),\n",
    "#            num.trees = 500) #use the Dots (...) for explicitly specifiying randomForest parameter\n",
    "# plot(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Deep Nets\n",
    "The following code snippet shows how to train a Deep Net. We use the ***keras*** framework on top of ***tensorflow***. \n",
    "\n",
    "We have enlarged the standard caret wrapper in order to also support **different layer architectures**, i.e. several fully connected layers of different size (e.g. size = \"10-8-5\" describes 3 consecutive layers comprising 10, 8 and 5 units resprecitvely). \n",
    "\n",
    "As already explained above, it is hard in case of tabular data to reach the predictive performance of boosted trees. Usually results (also for titanic data) are disappointing unless you put a high effort into tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = suppressMessages(suppressWarnings(\n",
    "      train(form      = as.formula(formula_notree),\n",
    "            data      = df.tune[c(\"target\",features_notree)],\n",
    "            method    = deepLearn,\n",
    "            trControl = ctrl_idx_nopar_fff,\n",
    "            metric    = \"AUC\",\n",
    "            tuneGrid  = expand.grid(size = c(\"10\",\"10-10\"),\n",
    "                                    lambda = c(0), dropout = 0.5,\n",
    "                                    batch_size = c(100), lr = c(1e-3),\n",
    "                                    batch_normalization = TRUE,\n",
    "                                    activation = c(\"relu\",\"elu\"),\n",
    "                                    epochs = 10),\n",
    "            preProc = c(\"center\",\"scale\"),\n",
    "            verbose = 0)\n",
    "))\n",
    "plot(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 2 Evaluate generalization gap for winner algorithm\n",
    "**Overfitting** is a big problem in Machine Learning. Usually the term overfitting describes the fact that your algorithms predictive performance decreases if you make it too flexible. You fit too near to the train data which decreases the bias but increases variance (\"bias-variance-tradeoff\") which leads to an overall degraded performance on your test data. This problem can be tackled by appropriate tuning.  \n",
    "\n",
    "But there is another overfitting problem that still remains in case you use a flexible algorithm (like xgboost for instance). Usually the performance on your training data is better than on your test data still after correct tuning. In fact to a certain extent this is a wished behavior as it shows you that the algorithm can model complex relationships between the input variable and the target. \n",
    "But if this ***generalization gap*** (i.e. the difference of predictive performance on training and test data) is getting too big you might find single weird predictions due to outliers in the target variable of the training data. For some businesses these false positives can be critical as they are hard or even impossible to justify (like in credit scoring).  \n",
    "You can attack the generaltization gap by using more data (see also paragraph \"4 Learning curve for winner algorithm\" below) or harder **regularization** of your algorithm (e.g. reduce the depth of trees in xgboost) but, as you have already reached the best hold-out performance due to appropriate tuning, this performance will then degrade.  \n",
    "Big generalization gaps are an overlooked problem in predictive modeling (utilizing Machine Learning), e.g. in kaggle competitions only the predictive performance on the test data is evaluated.  \n",
    "\n",
    "For a productive solution you should try to reduce the generalization gap, if it seems too big ... where \"big\" is highly subjective and depends on the business problem you try to solve. The following analysis supports you in finding better tuning parameters, i.e. parameters which **reduce the generalization gap but do not higly degrade the overall performance**.  \n",
    "E.g in the following plot you would choose the \"best\" model complexity, instead of the \"best ?\", even though it does have minimal test error (prediction error on test data) but better balances the generalization gap and the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(png); img = readPNG(\"DOKU/Bild2.png\"); par(mar = c(0,0,0,0)); options(repr.plot.width = 6, repr.plot.height = 3); plot.new();plot.window(0:1, 0:1); rasterImage(img, 0, 0, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "For the titanic data we see that reducing the depth of tress from 6 to 3 or increasing the *min_child_weight* value from 5 to 10 and using a smaller number of tress reduces the gap but keeps the test-AUC high. Again, keep in mind that due to the small number of data the results presented for titanic data are not very robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (usually undersample training data)\n",
    "df.gengap = df.tune #%>% sample_n(min(nrow(.),5e3))\n",
    "\n",
    "# Tune grid to loop over\n",
    "tunepar = expand.grid(nrounds = seq(100,2100,200), eta = c(0.01),\n",
    "                      max_depth = c(3,6), min_child_weight = c(5,10),\n",
    "                      colsample_bytree = c(0.3), subsample = c(0.7),\n",
    "                      gamma = c(0), alpha = c(0), lambda = c(1))\n",
    "\n",
    "# Calc generalization gap\n",
    "df.gengap_result = hmsPM::calc_gengap(df_data        = df.gengap,\n",
    "                                      formula_string = formula,\n",
    "                                      sparse         = TRUE,\n",
    "                                      method         = xgb_custom,\n",
    "                                      tune_grid      = tunepar,\n",
    "                                      cluster        = cl)\n",
    "\n",
    "# Plot generalization gap\n",
    "options(repr.plot.width = 10, repr.plot.height = 5)\n",
    "marrangeGrob(hmsPM::plot_gengap(df_gengap = df.gengap_result, \n",
    "                                metric    = \"AUC\", \n",
    "                                x         = \"nrounds\",\n",
    "                                color     = \"max_depth\", \n",
    "                                shape     = \"gamma\", \n",
    "                                facet     = \"min_child_weight ~ alpha + lambda\"),\n",
    "             ncol = 2, nrow = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 3 Simulation: Compare algorithms\n",
    "Once we have derived the tuning parameters (or at least a narrow parameter grid) we can compare the predictive performance of different algorithms. This is done in the following snippet. Actually for tabular data we usually just **compare Elastic Net (\"glmnet\") with Boosted Trees(\"xgb\")**.  \n",
    "\n",
    "The comparison is done by a simulation which comprises several random sample splits of training and test data and consecutive fitting and testing.\n",
    "The academic approach for this task would require a **nested cross-validation** (cv) where the inner cv-loop tunes the algorithm and the outer cv-loop compares the algorithms. Usually this takes too long, therefore we apply the comparison in a more pragmatic way as the aim is to get a rough impression of the performance gain of a sophisticated approach like Boosted Trees to a fully interpretable one like Elastic Net and not to compare to other benchmarks which you have to assure for an academic paper.  \n",
    "\n",
    "The following analysis also helps in **estimating the variablility** of the predictive performance of the algorithms. \n",
    "\n",
    "For titanic data there is no difference between the tested algorithms but for real life data you usually see that the gap between Boosted Trees and Elastic Net widens the more features you have. And keep in mind that we already pimped the Elastic Net by binning the metric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data sampling (for quick testing)\n",
    "df.sim = df.tune #%>% sample_n(min(nrow(.),5e3))\n",
    "\n",
    "# Define methods to run in simulation\n",
    "l.xgb = list(method         = xgb_custom, \n",
    "             formula_string = formula, \n",
    "             sparse         = TRUE,\n",
    "             tune_grid      = expand.grid(nrounds = 2100, eta = c(0.01),\n",
    "                                          max_depth = c(3), min_child_weight = c(10),\n",
    "                                          colsample_bytree = c(0.7), subsample = c(0.7),\n",
    "                                          gamma = 0, alpha = 0, lambda = 1))\n",
    "l.glmnet = list(method         = glmnet_custom, \n",
    "                formula_string = formula_binned, \n",
    "                sparse         = TRUE,\n",
    "                tune_grid      = expand.grid(alpha = 0,\n",
    "                                             lambda = 2^(seq(4, -10, -2))))\n",
    "\n",
    "# Simulate\n",
    "df.sim_result = hmsPM::calc_simulation(df_data           = df.sim,\n",
    "                                       n_sim             = 10,\n",
    "                                       metric            = \"AUC\",\n",
    "                                       sample_frac_train = 0.8,\n",
    "                                       sample_frac_test  = 0.5,\n",
    "                                       l_methods         = list(xgb = l.xgb,\n",
    "                                                                glmnet = l.glmnet))\n",
    "options(repr.plot.width = 6, repr.plot.height = 3)\n",
    "hmsPM::plot_simulation(df.sim_result, metric = \"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 4 Learning curve for winner algorithm\n",
    "\n",
    "In Section 2 we mentioned that the generalization gap can be tackled by using more data. This aspect can be visualized with a ***learning curve*** which shows the gap for different number of training records. More pecise, the model is trained for a fraction of the whole training data and the predictive performance is evaluated on this fraction of the training data as well as for the whole test data. Furthermore we can identify whether there is a **saturation in hold-out performance** which might lead to the decision to use less than all data for training in order to safe time and ressources.\n",
    "\n",
    "\n",
    "A phenotype learning curve looks like in the following png. It usually shows that with just a few training records the in-sample (i.e. train) performance on such a small set with a flexible algorithm is nearly perfect but very low for the whole out-of-sample data (i.e. test). Both effects get weaker the more training data we take.  \n",
    "Furthermore one can identify that above approx. 300k training records the predictive performance only slightly improves (i.e. it saturates) but the generalization gap is still getting narrower. Anyhow one might decide that this gap is acceptable and due to ressource constraints the training will be restricted to this number of records. \n",
    "\n",
    "\n",
    "One important hint: Usually the fraction of training is not just a random sample of the training data but a balanced sample **\"as long as balancing is possible\"**, e.g. think of training data with 1k records from the minority class and 10k records from the majority class, then up to a sample of 2000 records the sample is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(png); img = readPNG(\"DOKU/Bild1.png\"); par(mar = c(0,0,0,0)); options(repr.plot.width = 6, repr.plot.height = 6); plot.new();plot.window(0:1, 0:1); rasterImage(img, 0, 0, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For titanic we try balanced samples from 10% up to 100% of the whole training data. Again, for titanic you see some untypical behaviour due to the low number of data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data sampling (do NOT undersamle as this is done in calc_learningcurve; in fact you finally should not sample at all\n",
    "df.lc = df #%>% sample_n(min(nrow(.),5e3))\n",
    "\n",
    "# Tunegrid\n",
    "tunepar = expand.grid(nrounds = seq(100,500,200), eta = c(0.01),\n",
    "                      max_depth = c(3), min_child_weight = c(10),\n",
    "                      colsample_bytree = c(0.3), subsample = c(0.7),\n",
    "                      gamma = 0, alpha = 0, lambda = 1)\n",
    "\n",
    "# Calc lc\n",
    "df.lc_result = hmsPM::calc_learningcurve(df_data        = df.lc,\n",
    "                                         formula_string = formula,\n",
    "                                         sparse         = TRUE,\n",
    "                                         method         = xgb_custom,\n",
    "                                         tune_grid      = tunepar,\n",
    "                                         chunks_pct     = seq(20,100,10),\n",
    "                                         balanced       = TRUE,\n",
    "                                         metric         = \"AUC\")\n",
    "options(repr.plot.width = 6, repr.plot.height = 4)\n",
    "hmsPM::plot_learningcurve(df.lc_result, metric = \"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "We have finished the model comparison section. The goals of above processing are the following: \n",
    "- **decide for an algorithm**\n",
    "- **determine the tuning parmeter (or at least a fine grid of them)**\n",
    "- **get an impression of the variability of the algorithms predictive performance ...**\n",
    "- **... as well how it compares to other approaches**\n",
    "- **investigate whether we need all the training data**\n",
    "- **how big will be the generalization gap for the final model**\n",
    "\n",
    "\n",
    "All these aspects are important to know for the final model which will be further investigated in the forthcoming notebook \"3_interpretation_R.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
